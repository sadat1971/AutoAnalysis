{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c8c6610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "639f14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b82ac",
   "metadata": {},
   "source": [
    "## Following code can be used to test any algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928b09d",
   "metadata": {},
   "source": [
    "## Algorithms:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Naive Bayes\n",
    "3. Random Forest\n",
    "4. Adaboost\n",
    "5. XGboost\n",
    "6. FC\n",
    "7. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "2532294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self, train, test):\n",
    "        ## Train and test should be a dictionary, where train[\"X\"] should contain the features in numpy matrix format \n",
    "        ## mXn, where m is samples and n is features. the train[\"y\"] should contain the labels in (m,) shape\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        \n",
    "    def performance_evaluation(self, GT, pred, modelname=\"Empty\"):\n",
    "        acc = accuracy_score(GT, pred)\n",
    "        f1 = f1_score(GT, pred)\n",
    "        precision = precision_score(GT, pred)\n",
    "        recall = recall_score(GT, pred)\n",
    "        print(\"----- modelname is {}------\".format(modelname))\n",
    "        print(\"accuracy is {:.4f} and f1 score is {:.4f}\".format(acc, f1))\n",
    "        print(\"precision is {:.4f} and recall is {:.4f}\".format(precision, recall))\n",
    "        \n",
    "    def feature_importance(self, model):\n",
    "        ## feat importance\n",
    "        imp_acc = []\n",
    "        for i in range(self.train[\"X\"].shape[1]):\n",
    "            feat = self.train[\"X\"]\n",
    "            feat = feat.transpose()\n",
    "            np.random.shuffle(feat[i])\n",
    "            feat = feat.transpose()\n",
    "            train_pred = model.predict(feat)\n",
    "            acc = accuracy_score(self.train[\"y\"], train_pred)\n",
    "            imp_acc.append(acc)\n",
    "            if i%10==0:\n",
    "                print(\"we are on \", str(i))\n",
    "        df = pd.DataFrame()\n",
    "        df[\"featname\"] = ['f' + str(i) for i in range(self.train[\"X\"].shape[1])]\n",
    "        df[\"acc\"] = imp_acc\n",
    "        df = df.sort_values(by=['acc'], ascending=True)\n",
    "        return df\n",
    "        \n",
    "    def naive_bayes(self, compute_feat_importance=False):\n",
    "        df = pd.DataFrame()\n",
    "        gnb = GaussianNB()\n",
    "        y_pred = gnb.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"Gaussian NB\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(Optimized_model)\n",
    "        return y_pred, df\n",
    "    \n",
    "    def SVM(self, cval_range=[-2,2,4], gammaval_range=[-2, 2, 4], tune=False, nfolds=1, \n",
    "            compute_feat_importance=False):\n",
    "        df = pd.DataFrame()\n",
    "        if tune==True:\n",
    "            C = np.logspace(cval_range[0], cval_range[1], cval_range[2])\n",
    "            gamma = np.logspace(gammaval_range[0], gammaval_range[1], gammaval_range[2])\n",
    "            Param_tunable = {'C': C, 'gamma': gamma}\n",
    "            Optimized_model = GridSearchCV(svm.SVC(kernel='rbf'), \n",
    "                                           Param_tunable, cv=nfolds, verbose = True, \n",
    "                                           n_jobs = -1)\n",
    "        else:\n",
    "            Optimized_model = svm.SVC(kernel='rbf')\n",
    "        y_pred = Optimized_model.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"SVM\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(Optimized_model)\n",
    "        return y_pred, df\n",
    "    \n",
    "    def random_forest(self, Estimators=[80, 100, 120], tune=False, nfolds=1, compute_feat_importance=False):\n",
    "        df = pd.DataFrame()\n",
    "        if tune==True:\n",
    "            Param_tunable = {'n_estimators': Estimators}\n",
    "            Optimized_model = GridSearchCV(RandomForestClassifier(), Param_tunable, \n",
    "                                           cv=nfolds, verbose = 1, n_jobs = -1)\n",
    "            \n",
    "        else:\n",
    "            Optimized_model = RandomForestClassifier()\n",
    "        y_pred = Optimized_model.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"Random Forest\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(Optimized_model)\n",
    "        return y_pred, df\n",
    "    \n",
    "    def xgboost(self):\n",
    "        df = pd.DataFrame()\n",
    "        model = XGBClassifier()\n",
    "        y_pred = model.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"XGBoost\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(model)\n",
    "        return y_pred, df\n",
    "    \n",
    "    def adaboost(self, Estimators=[80, 100, 120], tune=False, nfolds=1, compute_feat_importance=False):\n",
    "        df = pd.DataFrame()\n",
    "        if tune==True:\n",
    "            Param_tunable = {'n_estimators': Estimators}\n",
    "            Optimized_model = GridSearchCV(AdaBoostClassifier(random_state=42), Param_tunable, \n",
    "                                           cv=nfolds, verbose = 1, n_jobs = -1)\n",
    "            \n",
    "        else:\n",
    "            Optimized_model = AdaBoostClassifier(random_state=42)\n",
    "        y_pred = Optimized_model.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"Ada Boost\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(Optimized_model)\n",
    "        return y_pred, df\n",
    "    \n",
    "    def logistic_regression(self, max_iter=100, compute_feat_importance=False):\n",
    "        df = pd.DataFrame()\n",
    "        model = LogisticRegression(random_state=0, max_iter=max_iter)\n",
    "        y_pred = model.fit(self.train[\"X\"], self.train[\"y\"]).predict(self.test[\"X\"])\n",
    "        self.performance_evaluation(self.test[\"y\"], y_pred, \"loisitic regression\")\n",
    "        if compute_feat_importance:\n",
    "            df = self.feature_importance(model)\n",
    "        return y_pred, df\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "876e9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/sadat/Documents/ml2_server/Han_Experiments/nlp-stuff-master/text_classification_HAN/\"\n",
    "train = dict()\n",
    "test = dict()\n",
    "train[\"X\"] = rev_train_X = np.load(path + \"results2/Review/Emb_han_Review_train.npy\")\n",
    "train[\"y\"] = rev_train_y = pd.read_pickle(path + \"data2/Review/train.pkl\").is_deception.values\n",
    "\n",
    "test[\"X\"] = rev_test_X = np.load(path + \"results2/Review/Emb_han_Review_test2.npy\")\n",
    "test[\"y\"] = rev_test_y = pd.read_pickle(path + \"data2/Review/test2.pkl\").is_deception.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a9028f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "030b38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is loisitic regression------\n",
      "accuracy is 0.6326 and f1 score is 0.6580\n",
      "precision is 0.6154 and recall is 0.7068\n"
     ]
    }
   ],
   "source": [
    "y = ml.logistic_regression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d63980e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----- modelname is Ada Boost------\n",
      "accuracy is 0.6314 and f1 score is 0.6576\n",
      "precision is 0.6140 and recall is 0.7080\n"
     ]
    }
   ],
   "source": [
    "y = ml.adaboost(Estimators=[80, 100, 120], tune=True, nfolds=5, compute_feat_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5dd5b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is SVM------\n",
      "accuracy is 0.6387 and f1 score is 0.6689\n",
      "precision is 0.6174 and recall is 0.7298\n"
     ]
    }
   ],
   "source": [
    "y = ml.SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1cde2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is Gaussian NB------\n",
      "accuracy is 0.6373 and f1 score is 0.6732\n",
      "precision is 0.6126 and recall is 0.7472\n"
     ]
    }
   ],
   "source": [
    "y, i = ml.naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "167ddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/sadat/Documents/ml2_server/Han_Experiments/nlp-stuff-master/text_classification_HAN/\"\n",
    "train = dict()\n",
    "test = dict()\n",
    "train[\"X\"] = rev_train_X = np.concatenate([np.load(path + \"results2/Review/Emb_han_Review_train.npy\"),\n",
    "                                           np.load(path + \"results2/Tweet/Emb_han_Review_train.npy\"),\n",
    "                                          np.load(path + \"results2/News/Emb_han_Review_train.npy\")], axis=1)\n",
    "                                           \n",
    "train[\"y\"] = rev_train_y = pd.read_pickle(path + \"data2/Review/train.pkl\").is_deception.values\n",
    "\n",
    "test[\"X\"] = rev_test_X = np.concatenate([np.load(path + \"results2/Review/Emb_han_Review_test2.npy\"),\n",
    "                                           np.load(path + \"results2/Tweet/Emb_han_Review_test2.npy\"),\n",
    "                                        np.load(path + \"results2/News/Emb_han_Review_test2.npy\")], axis=1)\n",
    "test[\"y\"] = rev_test_y = pd.read_pickle(path + \"data2/Review/test2.pkl\").is_deception.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0f51f5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is loisitic regression------\n",
      "accuracy is 0.6354 and f1 score is 0.6600\n",
      "precision is 0.6182 and recall is 0.7080\n",
      "we are on  0\n",
      "we are on  10\n",
      "we are on  20\n",
      "we are on  30\n",
      "we are on  40\n",
      "we are on  50\n",
      "we are on  60\n",
      "we are on  70\n",
      "we are on  80\n",
      "we are on  90\n",
      "we are on  100\n",
      "we are on  110\n",
      "we are on  120\n",
      "we are on  130\n",
      "we are on  140\n",
      "we are on  150\n",
      "we are on  160\n",
      "we are on  170\n",
      "we are on  180\n",
      "we are on  190\n"
     ]
    }
   ],
   "source": [
    "ml = model(train, test)\n",
    "y, df = ml.logistic_regression(max_iter=1000, compute_feat_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3e35e81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featname</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>f162</td>\n",
       "      <td>0.500561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>f163</td>\n",
       "      <td>0.500841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>f164</td>\n",
       "      <td>0.500911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>f165</td>\n",
       "      <td>0.501121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>f153</td>\n",
       "      <td>0.501542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f4</td>\n",
       "      <td>0.735109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f2</td>\n",
       "      <td>0.735319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.735389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f3</td>\n",
       "      <td>0.735950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f0</td>\n",
       "      <td>0.736020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    featname       acc\n",
       "162     f162  0.500561\n",
       "163     f163  0.500841\n",
       "164     f164  0.500911\n",
       "165     f165  0.501121\n",
       "153     f153  0.501542\n",
       "..       ...       ...\n",
       "4         f4  0.735109\n",
       "2         f2  0.735319\n",
       "1         f1  0.735389\n",
       "3         f3  0.735950\n",
       "0         f0  0.736020\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f1fa99a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is Gaussian NB------\n",
      "accuracy is 0.6348 and f1 score is 0.6692\n",
      "precision is 0.6116 and recall is 0.7388\n"
     ]
    }
   ],
   "source": [
    "ml = model(train, test)\n",
    "y, i = ml.naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dd683945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is XGBoost------\n",
      "accuracy is 0.6312 and f1 score is 0.6528\n",
      "precision is 0.6167 and recall is 0.6934\n"
     ]
    }
   ],
   "source": [
    "y = ml.xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f8161d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is XGBoost------\n",
      "accuracy is 0.6194 and f1 score is 0.6402\n",
      "precision is 0.6070 and recall is 0.6771\n"
     ]
    }
   ],
   "source": [
    "y = ml.xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml.SVM(cval_range=[-2,2,2], gammaval_range=[-2, 2, 2], tune=True, nfolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "82831fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- modelname is Random Forest------\n",
      "accuracy is 0.6300 and f1 score is 0.6510\n",
      "precision is 0.6161 and recall is 0.6900\n"
     ]
    }
   ],
   "source": [
    "y , i= ml.random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3331f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['f' + str(i) for i in range(64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "17d568df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.9/192.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from xgboost) (1.8.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from xgboost) (1.22.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
